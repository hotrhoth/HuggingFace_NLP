{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtwSsWGtw4fo"
      },
      "source": [
        "# Transformers, what can they do?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yHa2aNRfw4fr"
      },
      "source": [
        "Install the Transformers, Datasets, and Evaluate libraries to run this notebook."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "iBLdnHoYw4fr"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /Users/tramho/miniconda3/lib/python3.12/site-packages (3.2.0)\n",
            "Requirement already satisfied: evaluate in /Users/tramho/miniconda3/lib/python3.12/site-packages (0.4.3)\n",
            "Requirement already satisfied: transformers in /Users/tramho/miniconda3/lib/python3.12/site-packages (4.47.0)\n",
            "Requirement already satisfied: sentencepiece in /Users/tramho/miniconda3/lib/python3.12/site-packages (0.2.0)\n",
            "Requirement already satisfied: filelock in /Users/tramho/miniconda3/lib/python3.12/site-packages (from datasets) (3.16.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
            "Requirement already satisfied: pyarrow>=15.0.0 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from datasets) (17.0.0)\n",
            "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
            "Requirement already satisfied: pandas in /Users/tramho/miniconda3/lib/python3.12/site-packages (from datasets) (2.2.2)\n",
            "Requirement already satisfied: requests>=2.32.2 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from datasets) (2.32.2)\n",
            "Requirement already satisfied: tqdm>=4.66.3 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from datasets) (4.66.6.dev1+g729db6c)\n",
            "Requirement already satisfied: xxhash in /Users/tramho/miniconda3/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
            "Requirement already satisfied: multiprocess<0.70.17 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
            "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.9.0)\n",
            "Requirement already satisfied: aiohttp in /Users/tramho/miniconda3/lib/python3.12/site-packages (from datasets) (3.10.5)\n",
            "Requirement already satisfied: huggingface-hub>=0.23.0 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from datasets) (0.26.5)\n",
            "Requirement already satisfied: packaging in /Users/tramho/miniconda3/lib/python3.12/site-packages (from datasets) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from datasets) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from transformers) (2024.9.11)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from transformers) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (2.4.0)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.11.1)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->datasets) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets evaluate transformers sentencepiece"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow in /Users/tramho/miniconda3/lib/python3.12/site-packages (2.16.2)\n",
            "Requirement already satisfied: transformers[sentencepiece] in /Users/tramho/miniconda3/lib/python3.12/site-packages (4.47.0)\n",
            "Requirement already satisfied: filelock in /Users/tramho/miniconda3/lib/python3.12/site-packages (from transformers[sentencepiece]) (3.16.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from transformers[sentencepiece]) (0.26.5)\n",
            "Requirement already satisfied: numpy>=1.17 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from transformers[sentencepiece]) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from transformers[sentencepiece]) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from transformers[sentencepiece]) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from transformers[sentencepiece]) (2024.9.11)\n",
            "Requirement already satisfied: requests in /Users/tramho/miniconda3/lib/python3.12/site-packages (from transformers[sentencepiece]) (2.32.2)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from transformers[sentencepiece]) (0.21.0)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from transformers[sentencepiece]) (0.4.5)\n",
            "Requirement already satisfied: tqdm>=4.27 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from transformers[sentencepiece]) (4.66.6.dev1+g729db6c)\n",
            "Requirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from transformers[sentencepiece]) (0.2.0)\n",
            "Requirement already satisfied: protobuf in /Users/tramho/miniconda3/lib/python3.12/site-packages (from transformers[sentencepiece]) (4.25.5)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from tensorflow) (2.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from tensorflow) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from tensorflow) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.3.1 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from tensorflow) (0.3.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: setuptools in /Users/tramho/miniconda3/lib/python3.12/site-packages (from tensorflow) (75.6.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from tensorflow) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from tensorflow) (1.66.2)\n",
            "Requirement already satisfied: tensorboard<2.17,>=2.16 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from tensorflow) (2.16.2)\n",
            "Requirement already satisfied: keras>=3.0.0 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from tensorflow) (3.7.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow) (0.43.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers[sentencepiece]) (2024.9.0)\n",
            "Requirement already satisfied: rich in /Users/tramho/miniconda3/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (13.7.1)\n",
            "Requirement already satisfied: namex in /Users/tramho/miniconda3/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /Users/tramho/miniconda3/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from requests->transformers[sentencepiece]) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from requests->transformers[sentencepiece]) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from requests->transformers[sentencepiece]) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from requests->transformers[sentencepiece]) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install \"transformers[sentencepiece]\" tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-macos==2.16.2 in /Users/tramho/miniconda3/lib/python3.12/site-packages (2.16.2)\n",
            "Requirement already satisfied: tf-keras in /Users/tramho/miniconda3/lib/python3.12/site-packages (2.16.0)\n",
            "Requirement already satisfied: tensorflow==2.16.2 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from tensorflow-macos==2.16.2) (2.16.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos==2.16.2) (2.1.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos==2.16.2) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos==2.16.2) (24.3.25)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos==2.16.2) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos==2.16.2) (0.2.0)\n",
            "Requirement already satisfied: h5py>=3.10.0 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos==2.16.2) (3.12.1)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos==2.16.2) (18.1.1)\n",
            "Requirement already satisfied: ml-dtypes~=0.3.1 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos==2.16.2) (0.3.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos==2.16.2) (3.3.0)\n",
            "Requirement already satisfied: packaging in /Users/tramho/miniconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos==2.16.2) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos==2.16.2) (4.25.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos==2.16.2) (2.32.2)\n",
            "Requirement already satisfied: setuptools in /Users/tramho/miniconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos==2.16.2) (75.6.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos==2.16.2) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos==2.16.2) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos==2.16.2) (4.12.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos==2.16.2) (1.16.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos==2.16.2) (1.66.2)\n",
            "Requirement already satisfied: tensorboard<2.17,>=2.16 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos==2.16.2) (2.16.2)\n",
            "Requirement already satisfied: keras>=3.0.0 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos==2.16.2) (3.7.0)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from tensorflow==2.16.2->tensorflow-macos==2.16.2) (1.26.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from astunparse>=1.6.0->tensorflow==2.16.2->tensorflow-macos==2.16.2) (0.43.0)\n",
            "Requirement already satisfied: rich in /Users/tramho/miniconda3/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos==2.16.2) (13.7.1)\n",
            "Requirement already satisfied: namex in /Users/tramho/miniconda3/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos==2.16.2) (0.0.8)\n",
            "Requirement already satisfied: optree in /Users/tramho/miniconda3/lib/python3.12/site-packages (from keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos==2.16.2) (0.12.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2->tensorflow-macos==2.16.2) (2.0.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2->tensorflow-macos==2.16.2) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2->tensorflow-macos==2.16.2) (2.2.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from requests<3,>=2.21.0->tensorflow==2.16.2->tensorflow-macos==2.16.2) (2024.8.30)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos==2.16.2) (3.7)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos==2.16.2) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos==2.16.2) (3.0.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow==2.16.2->tensorflow-macos==2.16.2) (2.1.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos==2.16.2) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from rich->keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos==2.16.2) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /Users/tramho/miniconda3/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow==2.16.2->tensorflow-macos==2.16.2) (0.1.2)\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "pip install tensorflow-macos==2.16.2 tf-keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "Fst-yVBRw4fs",
        "outputId": "8ddea973-afb2-4a1b-a610-273e561016fa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-uncased-finetuned-sst-2-english and revision 714eb0f (https://huggingface.co/distilbert/distilbert-base-uncased-finetuned-sst-2-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "All PyTorch model weights were used when initializing TFDistilBertForSequenceClassification.\n",
            "\n",
            "All the weights of TFDistilBertForSequenceClassification were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\n",
            "Device set to use 0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9598048329353333}]"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\")\n",
        "classifier(\"I've been waiting for a HuggingFace course my whole life.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "2BzAeCInw4ft",
        "outputId": "7a680cf2-d62e-40f7-b10f-71b4bba1ede2"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "[{'label': 'POSITIVE', 'score': 0.9598048329353333},\n",
              " {'label': 'NEGATIVE', 'score': 0.9994558691978455}]"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "classifier(\n",
        "    [\"I've been waiting for a HuggingFace course my whole life.\", \"I hate this so much!\"]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "eCtsi7vLw4ft",
        "outputId": "e6f9eefd-ec2d-4101-df03-d7c83e950f1f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to FacebookAI/roberta-large-mnli and revision 2a8f12d (https://huggingface.co/FacebookAI/roberta-large-mnli).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "All PyTorch model weights were used when initializing TFRobertaForSequenceClassification.\n",
            "\n",
            "All the weights of TFRobertaForSequenceClassification were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForSequenceClassification for predictions without further training.\n",
            "Device set to use 0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'sequence': 'This is a course about the Transformers library',\n",
              " 'labels': ['education', 'business', 'politics'],\n",
              " 'scores': [0.9562345147132874, 0.02697216160595417, 0.016793321818113327]}"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"zero-shot-classification\")\n",
        "classifier(\n",
        "    \"This is a course about the Transformers library\",\n",
        "    candidate_labels=[\"education\", \"politics\", \"business\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "ECxxdcADw4ft",
        "outputId": "46e7702e-6d03-4242-8353-2a4afc342282"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to openai-community/gpt2 and revision 607a30d (https://huggingface.co/openai-community/gpt2).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
            "Device set to use 0\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'generated_text': 'In this course, we will teach you how to read and write code using Java-9 classes. We will use a Java EE project template and our own custom class to build out the core components of this project.'}]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\")\n",
        "generator(\"In this course, we will teach you how to\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "32hH5a8Rw4ft",
        "outputId": "1ca32f1b-f8fe-49df-a649-38ac103be7a9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "All PyTorch model weights were used when initializing TFGPT2LMHeadModel.\n",
            "\n",
            "All the weights of TFGPT2LMHeadModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFGPT2LMHeadModel for predictions without further training.\n",
            "Device set to use 0\n",
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'generated_text': 'In this course, we will teach you how to understand and understand the best tools for using this site. Learn more about this program by clicking here –'},\n",
              " {'generated_text': 'In this course, we will teach you how to create and perform your own tasks. But even the best of our ability, it is very important to'}]"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "generator = pipeline(\"text-generation\", model=\"distilgpt2\")\n",
        "generator(\n",
        "    \"In this course, we will teach you how to\",\n",
        "    max_length=30,\n",
        "    num_return_sequences=2,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UQCzre1iw4fu",
        "outputId": "df956463-9dd9-41fb-8748-ea2867eb3ebd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert/distilroberta-base and revision fb53ab8 (https://huggingface.co/distilbert/distilroberta-base).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "All PyTorch model weights were used when initializing TFRobertaForMaskedLM.\n",
            "\n",
            "All the weights of TFRobertaForMaskedLM were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFRobertaForMaskedLM for predictions without further training.\n",
            "Device set to use 0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'score': 0.19619588553905487,\n",
              "  'token': 30412,\n",
              "  'token_str': ' mathematical',\n",
              "  'sequence': 'This course will teach you all about mathematical models.'},\n",
              " {'score': 0.04052690789103508,\n",
              "  'token': 38163,\n",
              "  'token_str': ' computational',\n",
              "  'sequence': 'This course will teach you all about computational models.'}]"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "unmasker = pipeline(\"fill-mask\")\n",
        "unmasker(\"This course will teach you all about <mask> models.\", top_k=2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "6f9xKeNpw4fu",
        "outputId": "37b5e4a5-5c9b-4e02-a79d-ed1be4ba867a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to dbmdz/bert-large-cased-finetuned-conll03-english and revision 4c53496 (https://huggingface.co/dbmdz/bert-large-cased-finetuned-conll03-english).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "All PyTorch model weights were used when initializing TFBertForTokenClassification.\n",
            "\n",
            "All the weights of TFBertForTokenClassification were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForTokenClassification for predictions without further training.\n",
            "Device set to use 0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'entity_group': 'PER',\n",
              "  'score': 0.9981694,\n",
              "  'word': 'Sylvain',\n",
              "  'start': 11,\n",
              "  'end': 18},\n",
              " {'entity_group': 'ORG',\n",
              "  'score': 0.9796019,\n",
              "  'word': 'Hugging Face',\n",
              "  'start': 33,\n",
              "  'end': 45},\n",
              " {'entity_group': 'LOC',\n",
              "  'score': 0.9932106,\n",
              "  'word': 'Brooklyn',\n",
              "  'start': 49,\n",
              "  'end': 57}]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "ner = pipeline(\"ner\", grouped_entities=True)\n",
        "ner(\"My name is Sylvain and I work at Hugging Face in Brooklyn.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "JMMSnSVzw4fu",
        "outputId": "b7e2bb74-b880-414a-d5ca-5e518a21e2ed"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to distilbert/distilbert-base-cased-distilled-squad and revision 564e9b5 (https://huggingface.co/distilbert/distilbert-base-cased-distilled-squad).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "All PyTorch model weights were used when initializing TFDistilBertForQuestionAnswering.\n",
            "\n",
            "All the weights of TFDistilBertForQuestionAnswering were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForQuestionAnswering for predictions without further training.\n",
            "Device set to use 0\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'score': 0.694976270198822, 'start': 33, 'end': 45, 'answer': 'Hugging Face'}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "question_answerer = pipeline(\"question-answering\")\n",
        "question_answerer(\n",
        "    question=\"Where do I work?\",\n",
        "    context=\"My name is Sylvain and I work at Hugging Face in Brooklyn\",\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "jzMD02Tnw4fu",
        "outputId": "ba64f9c0-602d-400a-e154-9a9726bd846d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to google-t5/t5-small and revision df1b051 (https://huggingface.co/google-t5/t5-small).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
            "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
            "\n",
            "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n",
            "Device set to use 0\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "I0000 00:00:1734073492.273153 3553985 service.cc:145] XLA service 0x158ed2b30 initialized for platform Host (this does not guarantee that XLA will be used). Devices:\n",
            "I0000 00:00:1734073492.274816 3553985 service.cc:153]   StreamExecutor device (0): Host, Default Version\n",
            "2024-12-13 14:04:52.394609: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
            "I0000 00:00:1734073492.676113 3553985 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "[{'summary_text': 'the number of graduates in traditional engineering disciplines has declined . in most of the premier american universities engineering curricula now concentrate on and encourage largely the study of engineering science . rapidly developing economies such as China and India continue to encourage and advance the teaching of engineering .'}]"
            ]
          },
          "execution_count": 13,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "summarizer = pipeline(\"summarization\")\n",
        "summarizer(\n",
        "    \"\"\"\n",
        "    America has changed dramatically during recent years. Not only has the number of\n",
        "    graduates in traditional engineering disciplines such as mechanical, civil,\n",
        "    electrical, chemical, and aeronautical engineering declined, but in most of\n",
        "    the premier American universities engineering curricula now concentrate on\n",
        "    and encourage largely the study of engineering science. As a result, there\n",
        "    are declining offerings in engineering subjects dealing with infrastructure,\n",
        "    the environment, and related issues, and greater concentration on high\n",
        "    technology subjects, largely supporting increasingly complex scientific\n",
        "    developments. While the latter is important, it should not be at the expense\n",
        "    of more traditional engineering.\n",
        "\n",
        "    Rapidly developing economies such as China and India, as well as other\n",
        "    industrial countries in Europe and Asia, continue to encourage and advance\n",
        "    the teaching of engineering. Both China and India, respectively, graduate\n",
        "    six and eight times as many traditional engineers as does the United States.\n",
        "    Other industrial countries at minimum maintain their output, while America\n",
        "    suffers an increasingly serious decline in the number of engineering graduates\n",
        "    and a lack of well-educated engineers.\n",
        "\"\"\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "wmENK3kPw4fv",
        "outputId": "b097a9b0-f38b-4e22-8bd7-92a3ef9f73f6"
      },
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Could not load model Helsinki-NLP/opus-mt-fr-en with any of the following classes: (<class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForSeq2SeqLM'>, <class 'transformers.models.marian.modeling_tf_marian.TFMarianMTModel'>). See the original errors:\n\nwhile loading with TFAutoModelForSeq2SeqLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/Users/tramho/miniconda3/lib/python3.12/site-packages/transformers/pipelines/base.py\", line 289, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/tramho/miniconda3/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/tramho/miniconda3/lib/python3.12/site-packages/transformers/modeling_tf_utils.py\", line 3032, in from_pretrained\n    missing_keys, unexpected_keys, mismatched_keys = load_tf_weights(\n                                                     ^^^^^^^^^^^^^^^^\n  File \"/Users/tramho/miniconda3/lib/python3.12/site-packages/transformers/modeling_tf_utils.py\", line 952, in load_tf_weights\n    return load_function(\n           ^^^^^^^^^^^^^^\n  File \"/Users/tramho/miniconda3/lib/python3.12/site-packages/transformers/modeling_tf_utils.py\", line 994, in load_tf_weights_from_h5\n    saved_weights[name] = np.asarray(h5_layer_object[weight_name])\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n  File \"/Users/tramho/miniconda3/lib/python3.12/site-packages/h5py/_hl/dataset.py\", line 1085, in __array__\n    arr = numpy.zeros(self.shape, dtype=self.dtype if dtype is None else dtype)\n                                        ^^^^^^^^^^\n  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n  File \"/Users/tramho/miniconda3/lib/python3.12/site-packages/h5py/_hl/dataset.py\", line 563, in dtype\n    return self.id.dtype\n           ^^^^^^^^^^^^^\n  File \"h5py/h5d.pyx\", line 180, in h5py.h5d.DatasetID.dtype.__get__\n  File \"h5py/h5d.pyx\", line 183, in h5py.h5d.DatasetID.dtype.__get__\n  File \"h5py/h5t.pyx\", line 445, in h5py.h5t.TypeID.dtype.__get__\n  File \"h5py/h5t.pyx\", line 446, in h5py.h5t.TypeID.dtype.__get__\n  File \"h5py/h5t.pyx\", line 1087, in h5py.h5t.TypeFloatID.py_dtype\nValueError: Insufficient precision in available types to represent (31, 23, 8, 0, 23)\n\nwhile loading with TFMarianMTModel, an error is thrown:\nTraceback (most recent call last):\n  File \"/Users/tramho/miniconda3/lib/python3.12/site-packages/transformers/pipelines/base.py\", line 289, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/tramho/miniconda3/lib/python3.12/site-packages/transformers/modeling_tf_utils.py\", line 3032, in from_pretrained\n    missing_keys, unexpected_keys, mismatched_keys = load_tf_weights(\n                                                     ^^^^^^^^^^^^^^^^\n  File \"/Users/tramho/miniconda3/lib/python3.12/site-packages/transformers/modeling_tf_utils.py\", line 952, in load_tf_weights\n    return load_function(\n           ^^^^^^^^^^^^^^\n  File \"/Users/tramho/miniconda3/lib/python3.12/site-packages/transformers/modeling_tf_utils.py\", line 994, in load_tf_weights_from_h5\n    saved_weights[name] = np.asarray(h5_layer_object[weight_name])\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n  File \"/Users/tramho/miniconda3/lib/python3.12/site-packages/h5py/_hl/dataset.py\", line 1085, in __array__\n    arr = numpy.zeros(self.shape, dtype=self.dtype if dtype is None else dtype)\n                                        ^^^^^^^^^^\n  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n  File \"/Users/tramho/miniconda3/lib/python3.12/site-packages/h5py/_hl/dataset.py\", line 563, in dtype\n    return self.id.dtype\n           ^^^^^^^^^^^^^\n  File \"h5py/h5d.pyx\", line 180, in h5py.h5d.DatasetID.dtype.__get__\n  File \"h5py/h5d.pyx\", line 183, in h5py.h5d.DatasetID.dtype.__get__\n  File \"h5py/h5t.pyx\", line 445, in h5py.h5t.TypeID.dtype.__get__\n  File \"h5py/h5t.pyx\", line 446, in h5py.h5t.TypeID.dtype.__get__\n  File \"h5py/h5t.pyx\", line 1087, in h5py.h5t.TypeFloatID.py_dtype\nValueError: Insufficient precision in available types to represent (31, 23, 8, 0, 23)\n\n\n",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[0;32m----> 3\u001b[0m translator \u001b[38;5;241m=\u001b[39m \u001b[43mpipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtranslation\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mHelsinki-NLP/opus-mt-fr-en\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m translator(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCe cours est produit par Hugging Face.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m# [{'translation_text': 'This course is produced by Hugging Face.'}]\u001b[39;00m\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/pipelines/__init__.py:940\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    938\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    939\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m--> 940\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m \u001b[43minfer_framework_load_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    941\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmodel_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel_classes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    943\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    944\u001b[0m \u001b[43m        \u001b[49m\u001b[43mframework\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mframework\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    945\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    946\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mhub_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    947\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    948\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    950\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    951\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
            "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/transformers/pipelines/base.py:302\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m class_name, trace \u001b[38;5;129;01min\u001b[39;00m all_traceback\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    301\u001b[0m             error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile loading with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, an error is thrown:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtrace\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 302\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    303\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with any of the following classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_tuple\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. See the original errors:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m         )\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    307\u001b[0m     framework \u001b[38;5;241m=\u001b[39m infer_framework(model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n",
            "\u001b[0;31mValueError\u001b[0m: Could not load model Helsinki-NLP/opus-mt-fr-en with any of the following classes: (<class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForSeq2SeqLM'>, <class 'transformers.models.marian.modeling_tf_marian.TFMarianMTModel'>). See the original errors:\n\nwhile loading with TFAutoModelForSeq2SeqLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/Users/tramho/miniconda3/lib/python3.12/site-packages/transformers/pipelines/base.py\", line 289, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/tramho/miniconda3/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py\", line 564, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/tramho/miniconda3/lib/python3.12/site-packages/transformers/modeling_tf_utils.py\", line 3032, in from_pretrained\n    missing_keys, unexpected_keys, mismatched_keys = load_tf_weights(\n                                                     ^^^^^^^^^^^^^^^^\n  File \"/Users/tramho/miniconda3/lib/python3.12/site-packages/transformers/modeling_tf_utils.py\", line 952, in load_tf_weights\n    return load_function(\n           ^^^^^^^^^^^^^^\n  File \"/Users/tramho/miniconda3/lib/python3.12/site-packages/transformers/modeling_tf_utils.py\", line 994, in load_tf_weights_from_h5\n    saved_weights[name] = np.asarray(h5_layer_object[weight_name])\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n  File \"/Users/tramho/miniconda3/lib/python3.12/site-packages/h5py/_hl/dataset.py\", line 1085, in __array__\n    arr = numpy.zeros(self.shape, dtype=self.dtype if dtype is None else dtype)\n                                        ^^^^^^^^^^\n  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n  File \"/Users/tramho/miniconda3/lib/python3.12/site-packages/h5py/_hl/dataset.py\", line 563, in dtype\n    return self.id.dtype\n           ^^^^^^^^^^^^^\n  File \"h5py/h5d.pyx\", line 180, in h5py.h5d.DatasetID.dtype.__get__\n  File \"h5py/h5d.pyx\", line 183, in h5py.h5d.DatasetID.dtype.__get__\n  File \"h5py/h5t.pyx\", line 445, in h5py.h5t.TypeID.dtype.__get__\n  File \"h5py/h5t.pyx\", line 446, in h5py.h5t.TypeID.dtype.__get__\n  File \"h5py/h5t.pyx\", line 1087, in h5py.h5t.TypeFloatID.py_dtype\nValueError: Insufficient precision in available types to represent (31, 23, 8, 0, 23)\n\nwhile loading with TFMarianMTModel, an error is thrown:\nTraceback (most recent call last):\n  File \"/Users/tramho/miniconda3/lib/python3.12/site-packages/transformers/pipelines/base.py\", line 289, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/tramho/miniconda3/lib/python3.12/site-packages/transformers/modeling_tf_utils.py\", line 3032, in from_pretrained\n    missing_keys, unexpected_keys, mismatched_keys = load_tf_weights(\n                                                     ^^^^^^^^^^^^^^^^\n  File \"/Users/tramho/miniconda3/lib/python3.12/site-packages/transformers/modeling_tf_utils.py\", line 952, in load_tf_weights\n    return load_function(\n           ^^^^^^^^^^^^^^\n  File \"/Users/tramho/miniconda3/lib/python3.12/site-packages/transformers/modeling_tf_utils.py\", line 994, in load_tf_weights_from_h5\n    saved_weights[name] = np.asarray(h5_layer_object[weight_name])\n                          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n  File \"/Users/tramho/miniconda3/lib/python3.12/site-packages/h5py/_hl/dataset.py\", line 1085, in __array__\n    arr = numpy.zeros(self.shape, dtype=self.dtype if dtype is None else dtype)\n                                        ^^^^^^^^^^\n  File \"h5py/_objects.pyx\", line 54, in h5py._objects.with_phil.wrapper\n  File \"h5py/_objects.pyx\", line 55, in h5py._objects.with_phil.wrapper\n  File \"/Users/tramho/miniconda3/lib/python3.12/site-packages/h5py/_hl/dataset.py\", line 563, in dtype\n    return self.id.dtype\n           ^^^^^^^^^^^^^\n  File \"h5py/h5d.pyx\", line 180, in h5py.h5d.DatasetID.dtype.__get__\n  File \"h5py/h5d.pyx\", line 183, in h5py.h5d.DatasetID.dtype.__get__\n  File \"h5py/h5t.pyx\", line 445, in h5py.h5t.TypeID.dtype.__get__\n  File \"h5py/h5t.pyx\", line 446, in h5py.h5t.TypeID.dtype.__get__\n  File \"h5py/h5t.pyx\", line 1087, in h5py.h5t.TypeFloatID.py_dtype\nValueError: Insufficient precision in available types to represent (31, 23, 8, 0, 23)\n\n\n"
          ]
        }
      ],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")\n",
        "translator(\"Ce cours est produit par Hugging Face.\")\n",
        "\n",
        "# [{'translation_text': 'This course is produced by Hugging Face.'}]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "# https://huggingface.co/models"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Transformers, what can they do?",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
